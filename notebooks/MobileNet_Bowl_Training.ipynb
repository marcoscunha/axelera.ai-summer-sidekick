{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0484300-01c0-443f-887c-4f610d4fbf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../datasets/mobilenet_bowl\"\n",
    "MODEL_NAME = \"mobilenetv2_bowl_level_classifier\"\n",
    "CLASS_NAMES = [\"bowl_empty\", \"bowl_full\", \"bowl_half\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "983e3925-d230-4ef7-bcff-08e5229ab73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1, Loss: 0.9021\n",
      "Epoch 2, Loss: 0.4751\n",
      "Epoch 3, Loss: 0.3146\n",
      "Epoch 4, Loss: 0.2470\n",
      "Epoch 5, Loss: 0.2032\n",
      "Epoch 6, Loss: 0.1852\n",
      "Epoch 7, Loss: 0.1399\n",
      "Epoch 8, Loss: 0.1441\n",
      "Epoch 9, Loss: 0.1439\n",
      "Epoch 10, Loss: 0.1286\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#### TRAINING LOOP\n",
    "######################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define transforms for training and validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load datasets with the defined transforms\n",
    "train_dataset = datasets.ImageFolder(f\"{DATASET_PATH}/train\", transform=data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(f\"{DATASET_PATH}/val\", transform=data_transforms['val'])\n",
    "test_dataset = datasets.ImageFolder(f\"{DATASET_PATH}/test\", transform=data_transforms['test'])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load pretrained MobileNetV3 Large\n",
    "# model = models.mobilenet_v3_large(pretrained=True)\n",
    "# model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier for 3-class classification\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 3)  # 3 output classes\n",
    "# model.classifier[3] = nn.Linear(model.classifier[3].in_features, 3)  # 3 output classes\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), f\"{MODEL_NAME}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62a55acf-1b06-49d4-bd9d-7435a932b2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- Model Evaluation ---\n",
      "Accuracy: 0.9750\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 6  0  1]\n",
      " [ 0 16  0]\n",
      " [ 0  0 17]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  bowl_empty       1.00      0.86      0.92         7\n",
      "   bowl_full       1.00      1.00      1.00        16\n",
      "   bowl_half       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.97        40\n",
      "   macro avg       0.98      0.95      0.96        40\n",
      "weighted avg       0.98      0.97      0.97        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#### GENERATE METRICS \n",
    "######################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the trained MobileNetV2 model\n",
    "model = models.mobilenet_v2(weights=None)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(CLASS_NAMES))   # 3 output classes\n",
    "model.load_state_dict(torch.load(f\"{MODEL_NAME}.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Preprocess for MobileNetV2\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Class names\n",
    "# class_names = [\"bowl_empty\", \"bowl_full\", \"bowl_half\"]\n",
    "\n",
    "# Collect all test images and labels\n",
    "test_dir = f\"{DATASET_PATH}/test\"\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for idx, class_name in enumerate(CLASS_NAMES):\n",
    "    class_folder = os.path.join(test_dir, class_name)\n",
    "    images = [img for img in os.listdir(class_folder) if img.lower().endswith(('.jpg', '.png'))]\n",
    "    for img_name in images:\n",
    "        img_path = os.path.join(class_folder, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Warning: Could not read {img_path}\")\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img)\n",
    "        input_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        y_true.append(idx)\n",
    "        y_pred.append(predicted.item())\n",
    "\n",
    "# KPIs\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1ec180f-24e4-451b-a98a-83ebcaf0ae70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Predicted: bowl_full\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#### INFERENCE \n",
    "######################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the trained MobileNetV3 Large model\n",
    "# model = models.mobilenet_v3_large(weights=None)\n",
    "# model.classifier[3] = nn.Linear(model.classifier[3].in_features, 3)  # 3 classes\n",
    "model = models.mobilenet_v2(weights=None)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(CLASS_NAMES))  # 3 output classes\n",
    "model.load_state_dict(torch.load(f\"{MODEL_NAME}.pth\", map_location=device))\n",
    "model = model.to(device)  # Move model to the same device\n",
    "model.eval()\n",
    "\n",
    "# Preprocess for MobileNetV3 Large\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Class names\n",
    "# class_names = [\"bowl_empty\", \"bowl_full\", \"bowl_half\" ]\n",
    "\n",
    "# Load and preprocess an image of a bowl\n",
    "# bowl_img = cv2.imread(\"../datasets/mobilenet_bowl/train/bowl_half/2d20ca1b-PXL_20250703_095609216_0.jpg\")  # NumPy array\n",
    "bowl_folder = f\"{DATASET_PATH}/test/bowl_full\"\n",
    "bowl_images = [img for img in os.listdir(bowl_folder) if img.lower().endswith(('.jpg', '.png'))]\n",
    "random_img = random.choice(bowl_images)\n",
    "bowl_img = cv2.imread(os.path.join(bowl_folder, random_img))  # NumPy array\n",
    "# bowl_img = cv2.imread(\"../datasets/mobilenet_bowl/train/bowl_empty/0e37f9f6-PXL_20250628_155019197_0.jpg\")  # NumPy array\n",
    "\n",
    "bowl_img = cv2.cvtColor(bowl_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "bowl_img = Image.fromarray(bowl_img)  # Convert NumPy array to PIL Image\n",
    "bowl_img = preprocess(bowl_img).unsqueeze(0).to(device)  # Move input to the same device\n",
    "\n",
    "# Classify the bowl\n",
    "with torch.no_grad():\n",
    "    output = model(bowl_img)\n",
    "_, predicted = torch.max(output, 1)\n",
    "\n",
    "print(f\"Predicted: {CLASS_NAMES[predicted.item()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5283db0-2f0d-4ad0-8a52-e09d33ed885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#### EXPORT TO ONNX\n",
    "######################\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cpu\")   # force CPU for export\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Load the trained MobileNetV3 Large model\n",
    "# model = models.mobilenet_v3_large(weights=None)\n",
    "# model.classifier[3] = nn.Linear(model.classifier[3].in_features, 3)  # 3 classes\n",
    "model = models.mobilenet_v2(weights=None)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 3)  # 3 output classes\n",
    "ckpt = torch.load(f\"{MODEL_NAME}.pth\", map_location=device)\n",
    "model.load_state_dict(ckpt)\n",
    "model = model.to(device)  # Move model to the same device\n",
    "model.eval()\n",
    "\n",
    "# def patch_model(m: nn.Module):\n",
    "#     # Replace Hardswish/Hardsigmoid with export-friendly forms if present\n",
    "#     for name, child in list(m.named_children()):\n",
    "#         if isinstance(child, nn.Hardswish):\n",
    "#             setattr(m, name, nn.Sequential())  # identity; MobileNetV3 uses inplace hardswish in blocks\n",
    "#         elif isinstance(child, nn.Hardsigmoid):\n",
    "#             setattr(m, name, nn.Sigmoid())     # workable approximation for many exporters\n",
    "#         elif isinstance(child, nn.Conv2d) and getattr(child, \"padding_mode\", \"zeros\") != \"zeros\":\n",
    "#             # Force zero-pad mode\n",
    "#             child.padding_mode = \"zeros\"\n",
    "#         # If any Conv2d used padding='same', rewrite to explicit ZeroPad2d + Conv2d\n",
    "#         if isinstance(child, nn.Conv2d) and isinstance(child.padding, str):  # 'same' or 'valid'\n",
    "#             kH, kW = child.kernel_size if isinstance(child.kernel_size, tuple) else (child.kernel_size, child.kernel_size)\n",
    "#             sH, sW = child.stride if isinstance(child.stride, tuple) else (child.stride, child.stride)\n",
    "#             # compute SAME padding explicitly\n",
    "#             pad_h = max((sH - 1), 0) + max(kH - sH, 0)//2\n",
    "#             pad_w = max((sW - 1), 0) + max(kW - sW, 0)//2\n",
    "#             pad = nn.ZeroPad2d((pad_w, pad_w, pad_h, pad_h))\n",
    "#             new_conv = nn.Conv2d(child.in_channels, child.out_channels, (kH, kW),\n",
    "#                                  stride=(sH, sW), padding=0, dilation=child.dilation,\n",
    "#                                  groups=child.groups, bias=(child.bias is not None))\n",
    "#             new_conv.load_state_dict(child.state_dict(), strict=False)\n",
    "#             setattr(m, name, nn.Sequential(pad, new_conv))\n",
    "#         else:\n",
    "#             patch_model(child)\n",
    "\n",
    "# def patch_convs(m: nn.Module):\n",
    "#     for name, child in m.named_children():\n",
    "#         if isinstance(child, nn.Conv2d):\n",
    "#             if isinstance(child.padding, str):  # 'same' or 'valid'\n",
    "#                 # replace with explicit ZeroPad2d + Conv2d\n",
    "#                 kH, kW = child.kernel_size\n",
    "#                 sH, sW = child.stride\n",
    "#                 pad_h = max((sH - 1), 0) + max(kH - sH, 0)//2\n",
    "#                 pad_w = max((sW - 1), 0) + max(kW - sW, 0)//2\n",
    "#                 pad = nn.ZeroPad2d((pad_w, pad_w, pad_h, pad_h))\n",
    "#                 new_conv = nn.Conv2d(child.in_channels, child.out_channels,\n",
    "#                                      (kH, kW), stride=(sH, sW), padding=0,\n",
    "#                                      dilation=child.dilation, groups=child.groups,\n",
    "#                                      bias=(child.bias is not None))\n",
    "#                 new_conv.load_state_dict(child.state_dict(), strict=False)\n",
    "#                 setattr(m, name, nn.Sequential(pad, new_conv))\n",
    "#         else:\n",
    "#             patch_convs(child)\n",
    "\n",
    "# patch_model(model)\n",
    "# patch_convs(model)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    f\"{MODEL_NAME}.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    # dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    opset_version=17,   # check AxeleraAI docs for required opset\n",
    "    do_constant_folding=True,\n",
    "    # dynamo=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5329f3e0-5454-4bee-9500-c7db9f898683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model is valid!\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#### CHECK ONNX FORMAT\n",
    "######################\n",
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(f\"{MODEL_NAME}.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model is valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d51caadb-659c-4fe0-baf9-e2e3c6a554f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Add\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Add\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Add\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Add\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Add\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Add\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Add\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Add\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Add\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Add\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "Conv\n",
      "Conv\n",
      "Constant\n",
      "Constant\n",
      "Clip\n",
      "GlobalAveragePool\n",
      "Flatten\n",
      "Gemm\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#### CHECK THE ONNX OP\n",
    "######################\n",
    "import onnx\n",
    "\n",
    "model = onnx.load(f\"{MODEL_NAME}.onnx\")\n",
    "for node in model.graph.node:\n",
    "    print(node.op_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
